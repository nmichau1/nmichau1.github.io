<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <title>Resume - Start Bootstrap Theme</title>
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
        type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
    <link href="css/prism.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@11.9.0/styles/default.min.css">
    <script src="https://unpkg.com/@highlightjs/cdn-assets@11.9.0/highlight.min.js"></script>
    <script src="js/prism.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@11.9.0/styles/default.min.css">
    <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>

<body id="page-top">
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">
            <span class="d-block d-lg-none">Nicholas Michaud</span>
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
            aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span
                class="navbar-toggler-icon"></span></button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/index.html">Nicholas Michaud</a></li>
                <br> 
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/index.html#about">About</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/index.html#experience">Experience</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/index.html#education">Education</a></li>
                <!--<li class="nav-item"><a class="nav-link js-scroll-trigger" href="/index.html#skills">Skills</a></li>-->
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/index.html#interests">Projects</a></li>
            </ul>
        </div>
    </nav>
    <!-- Page Content-->
    <div class="container-fluid p-0">
        <section class="resume-section" id="about">
            <div class="resume-section-content">
                <h2 class="mb-0">
                    Marketing A/B Test of Statistical Significance
                </h2>
                <div class="subheading mb-5">
                    Python / A/B Test / Binomial Dist. / Proportionsw2

                </div>
                <p class="mb-5">For this analysis we'll be looking into a <a
                        href="https://www.kaggle.com/datasets/jackdaoud/marketing-data">dataset provided by iFood</a>, a
                    food delivery company based on Brazil.
                    There's a lot we could do with this dataset, but for no we'll be examining how Education impacts the
                    likelihood a user accepts a marketing campaign offer.

                    <br><br>
                    Starting out the data is stored in a csv so we'll format the data into a dataframe and take a look
                    at the columns. As you can see there's a lot to work with
                    and we won't be using most of the columns for now as we're not building a full predictive model
                    right now.
                </p>
                <pre><code class="language-python">df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv") 
df.columns
Index(['user id', 'test group', 'converted', 'total ads','most ads day', 'most ads hour'], dtype='object')</pre></code>
<p class="mb-5"> This is the results of an A/B test so we're mostly concerned with 'test group' as it identifies which experimental group the user was a part of.
     We're also concerned with 'converted' as our variable of interest. It's a 1/0 dichotmous variable so we're not concerned with revenue of any sort.
     
    For the sake of this example, we'll assume all conversions are valued the same. We'll do the same with ad cost. We'll keep things simple and just examine the incrementality of the campaign. 
    
     As part of this, we'll want to look into the details of each group individually. The easiest way to do this is to create one dataframe for the experimental group and the control group.
</p>
<pre><code class="language-python">
#a/b test & control results split into two dataframes  
df_A = df[df["test group"] != "ad"]
df_B = df[df["test group"] == "ad"]
</code></pre>
<p class="mb-5"> Now since the only variable of concern here is conversions the following is all true:

    <ul>
        <li> our experiment counts are finite </li>
        <li> all users in our experiment are independent of eachother </li>
        <li> Our outcome (conversions) is dichotomous </li>
        <li> our probability of success/failure is constant for each trial in the test </li>
    </ul>

    By meeting all of these requirements, we can consider a single user as a bernoulli trial and our full experiment as a series of independent bernoulli trials.
    This means our data is best modelled by a binomial distribution.

    Now how does this help us? Modeling to a binomial distribution gives us more tools and insight that we would have otherwise. 

    First and foremost it allows us to define the probability of conversion as a binomial random variable, which comes with some attributes.

    The mean of of our random variable X is equal to \( E(X) = np \)

    The standard deviation is \(\sigma = \sqrt{(np)*(1-p)} \).

    So just like that we have formulas to represent our distribution.
    Now that we know this, in order to graph our probability mass function, we can just plug and chug.

     </p>
     <pre><code class="language-python">
     


#We'll start by graphing out our probability mass function for our experimental group

#number of bernoulli trials
n_b = len(df_ad)

# proportion of converted users(i.e probability)
p_b = len(df_ad[df_ad["converted"]==True])/len(df_ad)

#range of values to be graphed on
r_values_B = list(range(n+1))

plt.plot(r_values,binom.pmf(r_values,n,p))


### GRAPH###

We can do the same for our control group as well

n_A = len(df_A)
p_A = len(df_A[df_A["converted"]==True])/len(df_A)
r_values_A = list(range(n_A+1))

std = math.sqrt((n*p)*(1-p))


plt.plot(r_values,binom.pmf(r_values,n,p))
plt.plot(c_r_values,binom.pmf(c_r_values,c_n,c_p))

plt.xlim(0,20000)

     </code></pre>


Now that we have a basic intuition of our distributions here we can move onto hypothesis testing.

We'll be using a classic test seen below:
<br><br>
 \( \text{Null Hypothesis } (H_0) \text{: There is no significant difference between the two groups: } p_{test} = p_{control} \)
 \( \text{Alternate Hypothesis } (H_a) \text{: There is significant difference between the two groups: } p_{test} \neq p_{control} \)
<br><br>


Taking a quick look at our overall count we can see N is quite high. Given this we assume that the distribution is essentially normal. 
Even with a bit lower probability, this still holds true given our large sample size.
 
<pre><code class="language-python">  
    print(len(df))
    print(len(df_b))
    print(len(df_A))
</code></pre>

Using this we can build estimates for a shared population distribution given the null hypothesis of no difference

 To do so, we'll start by calculating the pooled proportion between control and test groups. 
 Since our assumption is no difference, it makes sense that the shared "parent" population would be best estimated using both distributions.

The pooled proportion is an estimate for the probability of the shared population distribution between A/B
\(\Large p_{pooled} = \frac{(p_{test} + p_{control})}{2} \)

In other words p-pool acts as our estimate for \(E(H_0)\) aka the center of our null distribution.

Next we just need to calculate standard error of the null distribution. Given we're working with binomial distributions here it as below:

 \(SE = \sqrt{ \Large\frac{p_{pooled} * (1- p_{pooled})}{(n_{test} + n_{control})}} \)

This gives us the attributes of our null distribution we need.

Now how do we connec our binomial distributions and the normal null distribution?

As before, our A/B binomial distributions approximately resemble a normal distribution with the follow attributes:

#mu = p-hat
#std = estimated via standard error

Using this conversion, we can essentially work with only normal distributions now.


Now to take a pause before we move forward with our hypothesis testing, we'll next want to do a power analysis to ensure our chance of committing type II errors is tolerable.

Theres no point in calculating our test statistic if our sample size is too small for the results to be actionable yet.



Putting our information into a power analysis calculator gives us around 14k total needed to hit .80 power. We should be safe to move forward here.




#now we can do a simple Z test

#calculate the z statistic
# Z =(p-hat1 - p-hat2/SE)
#we're calculating the difference between the two proportions by how many standard errors we expect.
# our goal is to see how unlikely this distance would be to occur in our null hypothesis by chance

# we'll be using a one-side test here with A = .05

# this gives us our acceptable chance of commiting a type I error




# now based on this we're able to reject /not able to rject the null


# log effect size











</pre></code>

                <p class="mb-5"> We'll be focusing around education as a factor in AcceptedCMPOverall here. As you can
                    see in the columns list above, there's 5 mutually exclusive dichotomous columns to work with.
                    The data doesn't have any NAs so we can move ahead

            
            </div>
        </section>



</body>