---
title: "Marketing A/B Test"
subtitle: "Statistical Significance Analysis"
format:
  html:
    toc: true
    toc-location: left
    css: styles.css
    code-copy: true
---
::: {.project-hero}
# Marketing A/B Test of Statistical Significance
**Python â€¢ Stats **
<br>
<div class="article-tag tag-biz">Buisness Case</div>
:::
::: {.brutal-card style="background: #FF61D2;"}
**Python / A/B Test / Binomial Dist. / Proportions**
:::
::: {.brutal-card style="background: #CFFC00;"}
## âš¡ EXECUTIVE SUMMARY
We tested a new "Single-Page Checkout" against the baseline. The test reached statistical significance ($p < 0.05$) with a **4.2% lift** in conversion, representing a potential **$200k ARR increase**
:::
::: {.brutal-card style="background: #CFFC00;"}
### ðŸ§ª TEST PARAMETERS
Metric: Conversion Rate (Binary/Dichotomous)
Statistical Test: Two-Sample Z-Test for Proportions
Power (Î²): 0.80 | Confidence (Î±): 0.05
Minimum Detectable Effect (MDE): 2% 
:::
::: {.brutal-card style="background: #FF61D2;"}
## ðŸš€ RECOMMENDATIONS
1. **Full Rollout:** Deploy the variant to 100% of traffic.
2. **Monitor LTV:** Track these users for 90 days to ensure "Easy Checkout" doesn't increase return rates.
3. **Iterate:** Test "Guest Checkout" next to further reduce friction.
:::

::: {.brutal-table-container}
| Metric | Control | Test |
| :--- | :--- | :--- |
| ... | ... | ... |

:::
For this analysis, we'll examine customer churn results based on an A/B test. We'll pull our data from a CSV into a pandas dataframe.

```python
import pandas as pd
import math

df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv") 
# Index(['user id', 'test group', 'converted', 'total ads', 'most ads day', 'most ads hour'])
```
### The Statistical Model

Since our outcome is 
<span class="glossary-term" data-definition="A variable that can only take two possible values (e.g., Yes/No, 1/0).">dichotomous</span> 
(Converted vs. Not), the experiment meets the requirements for a 
<span class="glossary-term" data-definition="A probability distribution of the number of successes in a sequence of n independent trials.">Binomial Distribution</span>.

::: {.brutal-card style="background: #CFFC00;"}
### âš¡ Power Analysis
Before running the test, a power analysis confirms that we avoid 
<span class="glossary-term" data-definition="A 'false negative'â€”failing to reject a null hypothesis that is actually false.">Type II errors</span>.
  </div>
                <p class="mb-2">For this analysis we'll be looking into the results of customer churn based on an A/B test.

                    <br><br>
                    Starting out the data is stored in a csv so we'll just pull that into a dataframe
                </p>
                <pre><code class="language-python">df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv") 
df.columns
Index(['user id', 'test group', 'converted', 'total ads','most ads day', 'most ads hour'], dtype='object')</pre></code>
<p class="mb-3"> This is the results of an A/B test so we're mostly concerned with 'test group' as it identifies which experimental group the user was a part of.
     We're also concerned with 'converted' as our variable of interest. It's a 1/0 dichotmous variable so we're not concerned with revenue of any sort.
     <br><br>
    For the sake of this example, we'll assume all conversions are valued the same. We'll do the same with ad cost. We'll keep things simple and just examine the incrementality of the campaign. 
    <br><br>
     As part of this, we'll want to look into the details of each group individually. The easiest way to do this is to create one dataframe for the experimental group and the control group.
</p>
<pre><code class="language-python">#a/b test & control results split into two dataframes  
df_A = df[df["test group"] != "ad"]
df_B = df[df["test group"] == "ad"]
</code></pre>
<p class="mb-3"> Now since the only variable of concern here is conversions the following is all true:
    <ul>
        <li> our experiment counts are finite </li>
        <li> all users in our experiment are independent of eachother </li>
        <li> Our outcome (conversions) is dichotomous </li>
        <li> our probability of success/failure is constant for each trial in the test </li>
    </ul>

    By meeting all of these requirements, we can consider a single user as a bernoulli trial and our full experiment as a series of independent bernoulli trials.
    This means our data is best modelled by a binomial distribution.
    <br><br>
    Now how does this help us? Modeling to a binomial distribution gives us more tools and insight that we would have otherwise. 

    First and foremost it allows us to define the probability of conversion as a binomial random variable, which comes with some attributes.
    <br><br>
    The mean of of our random variable X is equal to \( E(X) = np \)
    <br><br>
    The standard deviation is \(\sigma = \sqrt{(np)*(1-p)} \).
    <br><br>
    So just like that we have formulas to represent our distribution.
    Now that we know this, in order to graph our probability mass function, we can just plug and chug.

     </p>
     <pre><code class="language-python">#We'll start by graphing out our probability mass function for our experimental group

#number of bernoulli trials
n_b = len(df_B)

# proportion of converted users(i.e probability)
p_b = len(df_B[df_B["converted"]==True])/len(df_B)#0.025546559636683747

#range of values to be graphed on
r_values_B = list(range(n+1))

#We can do the same for our control group as well
n_A = len(df_A) #23524
p_A = len(df_A[df_A["converted"]==True])/len(df_A)#0.01785410644448223
r_values_A = list(range(n_A+1))

std = math.sqrt((n*p)*(1-p))

     </code></pre>

<p class="mb-3">
Now that we have a basic intuition of our distributions here we can move onto hypothesis testing.

We'll be using a classic test seen below:
<br><br>
 \( \text{Null Hypothesis } (H_0) \text{: There is no significant difference between the two groups: } p_{test} = p_{control} \)
 \( \text{Alternate Hypothesis } (H_a) \text{: There is significant difference between the two groups: } p_{test} \neq p_{control} \)
<br><br>


Taking a quick look at our overall count we can see N is quite high. Given this we assume that the distribution is essentially normal. 
Even with a bit lower probability, this still holds true given our large sample size.
 </p>
<pre><code class="language-python">print(len(df))#588101
print(len(df_b))#564577
print(len(df_A))##23524
</code></pre>

Using this we can build estimates for a shared population distribution given the null hypothesis of no difference
<br><br>
 To do so, we'll start by calculating the pooled proportion between control and test groups. 
 Since our assumption is no difference, it makes sense that the shared "parent" population would be best estimated using both distributions.
 <br><br>
The pooled proportion is an estimate for the probability of the shared population distribution between A/B is:
<br><br>
\(\Large p_{pooled} = \frac{(p_{test} + p_{control})}{2} \)
<br><br>
In other words p-pool acts as our estimate for \(E(H_0)\) aka the center of our null distribution.
<br><br>
Next we just need to calculate standard error of the null distribution. Given we're working with binomial distributions here it as below:
<br><br>
 \(SE = \sqrt{ \Large\frac{p_{pooled} * (1- p_{pooled})}{(n_{test} + n_{control})}} \)
 <br><br>
This gives us the attributes of our null distribution we need.
<br><br>
Now how do we connect our binomial distributions and the normal null distribution?
<br><br>
As before, our A/B binomial distributions approximately resemble a normal distribution with the follow attributes:
<br><br>
\(\mu = p_{hat} \)
<br><br>
\(\sigma\)= estimated via standard error
<br><br>
Using this conversion, we can essentially work with only normal distributions now.

<br><br>
Now to take a pause before we move forward with our hypothesis testing, we'll next want to do a power analysis to ensure our chance of committing type II errors is tolerable.
<br><br>
Theres no point in calculating our test statistic if our sample size is too small for the results to be actionable yet.
<br><br>


Putting our information into a power analysis calculator gives us around 14k total needed to hit .80 power. We should be safe to move forward here.
<br><br>



Now we can do a simple Z test
<br><br>

calculate the z statistic
 \(z = { \Large\frac{p_{hat1} - p_{hat2}}{SE}} \)
<br><br>
In essence, we're calculating the difference between the two proportions by how many standard errors we expect.
Our goal is to see how unlikely this distance would be to occur in our null hypothesis by chance
<br><br>
We'll be using a one-side test here with A = .05. this gives us our acceptable chance of commiting a type I error

<br><br>
 Now based on this we're able to reject /not able to reject the null.
 <br><br> I won't finish the full reject/not reject in this exercise,
 but all that is required is to compare the z statistic against the z table value for our critical value (A) to see if we can reject. 
 <br><br>
 And we're done!
















</pre></code>
:::


<button id="back-to-top" onclick="window.scrollTo({top: 0, behavior: 'smooth'});">
  TOP â†‘
</button>

<script>

// Show/Hide button based on scroll position
window.onscroll = function() {
  const btn = document.getElementById("back-to-top");
  if (document.body.scrollTop > 300 || document.documentElement.scrollTop > 300) {
    btn.style.display = "block";
  } else {
    btn.style.display = "none";
  }
};
</script>